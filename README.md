# Improving Semantic Understanding in Speech Language Models via Brain-tuning


[Paper ICLR Page](https://iclr.cc/virtual/2025/poster/30063) | [arXiv](https://arxiv.org/abs/2410.09230) | [Poster](https://iclr.cc/virtual/2025/poster/30063)

Code for the paper: 

Omer Moussa, Dietrich Klakow, and Mariya Toneva. Improving Semantic Understanding in Speech Language Models via Brain-tuning. In _International Conference on Learning Representations (ICLR)_, 2025. 

## Abstract
Speech language models align with human brain responses to natural language to an impressive degree. However, current models rely heavily on low-level speech features, indicating they lack brain-relevant semantics which limits their utility as model organisms of semantic processing in the brain. In this work, we address this limitation by inducing brain-relevant bias directly into the models via fine-tuning with fMRI recordings of people listening to natural stories--a process we name brain-tuning. After testing it on 3 different pretrained model families, we show that brain-tuning not only improves overall alignment with new brain recordings in semantic language regions, but also reduces the reliance on low-level speech features for this alignment. Excitingly, we further show that brain-tuning leads to 1) consistent improvements in performance on semantic downstream tasks and 2) a representational space with increased semantic preference. Our results provide converging evidence, for the first time, that incorporating brain signals into the training of language models improves the modelsâ€™ semantic understanding.

## How to Cite

If you use any of the insights or ideas from this poster in your work, please cite it as follows:


# Contributors
[Omer Moussa](https://www.mpi-sws.org/people/omoussa/) (omoussa@mpi-sws.org) - Corresponding Author

[Prof. Dietrich Klakow](https://www.lsv.uni-saarland.de/people/dietrich-klakow/) (dietrich.klakow@lsv.uni-saarland.de)

[Prof. Mariya Toneva](https://mtoneva.com/) (mtoneva@mpi-sws.org)

